torch>=2.4.0
transformers>=4.51.0
xxhash

[:sys_platform == "linux" and python_version == "3.11"]
triton>=3.0.0

[:sys_platform == "linux" and python_version == "3.11" and platform_machine == "x86_64"]
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.12/flash_attn-2.8.3+cu128torch2.10-cp311-cp311-linux_x86_64.whl

[:sys_platform == "win32" and python_version == "3.11"]
triton-windows<3.4,>=3.0.0

[:sys_platform == "win32" and python_version == "3.11" and platform_machine == "AMD64"]
flash-attn @ https://github.com/sdbds/flash-attention-for-windows/releases/download/2.8.2/flash_attn-2.8.2+cu128torch2.7.1cxx11abiFALSEfullbackward-cp311-cp311-win_amd64.whl
